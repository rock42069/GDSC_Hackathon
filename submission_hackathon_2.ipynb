{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils \n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import scipy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 26\n",
    "\n",
    "images = [cv2.imread(path, 0) for path in glob.glob( r\"C:\\Users\\Trijal Srivastava\\OneDrive\\Desktop\\VS CODE\\Hackathon GDIS\\Hackathon_2_Dataset\\img\\*.png\")]\n",
    "images = np.asarray(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1716, 36, 36)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"C:\\Users\\Trijal Srivastava\\OneDrive\\Desktop\\VS CODE\\Hackathon GDIS\\Hackathon_2_Dataset\\img\"\n",
    "\n",
    "# new_name_ = 'image_'\n",
    "\n",
    "# files = os.listdir(path)\n",
    "\n",
    "# for i, old_name in enumerate(files):\n",
    "#     new_name = f\"{new_name_}{i}.png\"\n",
    "#     os.rename(os.path.join(path,old_name), os.path.join(path,new_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Trijal Srivastava\\OneDrive\\Desktop\\VS CODE\\Hackathon GDIS\\Hackathon_2_Dataset\\img\"\n",
    "num_classes = 26\n",
    "image_size = 32\n",
    "\n",
    "label_encoder = {i: torch.eye(num_classes)[i] for i in range(num_classes)}\n",
    "\n",
    "transform= transforms.Compose([\n",
    "                               # Here i have added two lines of code since our dataset had images of 36*36 pixels\n",
    "                               # We are resizing it to 64*64, as the original paper had images of 64*64 in it.\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,))\n",
    "                           ])\n",
    "\n",
    "full_dataset = []\n",
    "for i in range(1716):\n",
    "    alphabet_index = i % 26\n",
    "    \n",
    "    image_path = os.path.join(path, f'image_{i}.png')\n",
    "    image = transform(Image.open(image_path))\n",
    "    label = torch.tensor(alphabet_index)\n",
    "    one_hot_label = label_encoder[alphabet_index]\n",
    "    full_dataset.append((image, label, one_hot_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlphabetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.images = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            class_images = [os.path.join(class_path, img_name) for img_name in os.listdir(class_path)]\n",
    "            self.images.extend(class_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       img_path = self.images[idx]\n",
    "       class_name = os.path.basename(os.path.dirname(img_path))\n",
    "       class_idx = self.class_to_idx[class_name]\n",
    "       image = Image.open(img_path)\n",
    "\n",
    "       if self.transform:\n",
    "        image = self.transform(image)\n",
    "\n",
    "       return image, class_idx\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), \n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = AlphabetDataset(root_dir='/content/drive/MyDrive/Hackathon_2_Dataset_final', transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=66, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workers = 2\n",
    "batch_size = 26\n",
    "num_classes = 26\n",
    "image_size = 64\n",
    "\n",
    "nc = 1\n",
    "\n",
    "nz = 100 \n",
    "\n",
    "ngf = 64\n",
    "\n",
    "ndf = 64\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "beta1 = 0.5\n",
    "\n",
    "CUDA = True\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14335f125f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# dataloader = torch.utils.data.DataLoader(full_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "# device = torch.device(\"cpu\")\n",
    "# real_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64],padding=2,normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA = CUDA and torch.cuda.is_available()\n",
    "# print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "# if CUDA:\n",
    "#     print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "# if CUDA:\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "# device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "# cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    #takes class name of m as input\n",
    "    classname = m.__class__.__name__\n",
    "    # if its a conv layer initialise weights accordingly\n",
    "    if classname.find('Conv')!=-1:\n",
    "        nn.init.normal_(m.weight.data, 0.0,0.02)\n",
    "    # if its a batchnorm layer initialise weights accordingly    \n",
    "    elif classname.find('BatchNorm')!= -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0,0.02)\n",
    "        #bias terms are set to 0\n",
    "        nn.init.constant_(m.bias.data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, nz)\n",
    "        self.main = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(nz + ngf*8, ngf*8,4,1,0,bias=False),\n",
    "            nn.BatchNorm2d(ngf*8),\n",
    "            nn.ReLU(True),\n",
    "            # 512*4*4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # 256*8*8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # 128*16*16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # 64*32*32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # 3*64*64`\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        label_embedding = self.label_embedding(labels).view(labels.size(0), labels.size(1), 1, 1)\n",
    "        z = torch.cat((label_embedding, input), 1)\n",
    "        return self.main(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (label_embedding): Embedding(26, 100)\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(612, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator().to(device)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, nc * 64 * 64) \n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(nc+ndf, ndf, 4,2,1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 64*32*32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 128*16*16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 256*8*8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 512*4*4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            # Final output is a single integer on which sigmoid is applied\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        label_embedding = self.label_embedding(labels).view(labels.size(0), labels.size(1), 64, 64)\n",
    "        x = torch.cat((input, label_embedding), 1) \n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (label_embedding): Embedding(26, 12288)\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(67, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_labels(batch_size):\n",
    "    return torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "\n",
    "#we will use binary cross entropy loss for updating the weights and calculating loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1,1, device = device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "#update the weights of the discriminator during training\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr , betas=(beta1,0.999))\n",
    "#update the weights of the generator during training\n",
    "optimizerG = optim.Adam(netG.parameters(), lr= lr , betas=(beta1,0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Trijal Srivastava\\OneDrive\\Desktop\\VS CODE\\Hackathon GDIS\\submission_hackathon_2.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m label_r \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b_size,), real_label, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Forward pass real batch through D\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m netD(images, labels)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Calculate loss on all-real batch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m errD_real \u001b[39m=\u001b[39m criterion(output, label_r)\n",
      "File \u001b[1;32mc:\\Users\\Trijal Srivastava\\Downloads\\anaconda3new\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Trijal Srivastava\\OneDrive\\Desktop\\VS CODE\\Hackathon GDIS\\submission_hackathon_2.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, labels):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     label_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_embedding(labels)\u001b[39m.\u001b[39mview(labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), labels\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m), \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39minput\u001b[39m, label_embedding), \u001b[39m1\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Trijal%20Srivastava/OneDrive/Desktop/VS%20CODE/Hackathon%20GDIS/submission_hackathon_2.ipynb#X52sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain(x)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, (images, labels, one_hot_labels) in enumerate(dataloader, 0):\n",
    "\n",
    "        netD.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.argmax(one_hot_labels, dim=1)  # Convert one-hot encoding to class indices\n",
    "\n",
    "        one_hot_labels = one_hot_labels.to(device)\n",
    "        \n",
    "         \n",
    "        b_size = images.size(0)\n",
    "        label_r = torch.full((b_size,), real_label, dtype=torch.float, device=device) \n",
    "         \n",
    "        # Forward pass real batch through D\n",
    "        output = netD(images, labels).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label_r)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        labels_f = generate_random_labels(b_size)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise,labels_f)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
